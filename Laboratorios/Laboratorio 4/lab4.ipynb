{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "864d0f72"
      },
      "source": [
        "### LABORATORIO 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "20Q29kX8SxgJ"
      },
      "outputs": [],
      "source": [
        "# LIBRERIAS\n",
        "import os #Para mejor navegacion en directorios\n",
        "import numpy as np #Para hacer cálculos matemáticos\n",
        "from matplotlib import pyplot #Para hacer gráficos\n",
        "%matplotlib inline\n",
        "from scipy import optimize #Para entrenar nuestros optimizadores\n",
        "import pandas as pd #Para leer datasets\n",
        "from sklearn.preprocessing import LabelEncoder #Para convertir palabras en números o valores maquina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hhRjL2ptSxgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9726004b-8ee2-49bd-9609-9c80efec74cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma original: (81048, 55)\n",
            "Forma 20k: (19998, 55)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-258965537.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.sample(n=min(len(g), max(1, int(len(g) * N / len(df)))), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases originales: [1. 2. 3. 4. 5. 6. 7.]\n",
            "Clases codificadas (0..K-1): [np.int8(0), np.int8(1), np.int8(2), np.int8(3), np.int8(4), np.int8(5), np.int8(6)]\n",
            "Forma: (15998, 54) (4000, 54)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "CSV_PATH = \"/content/covtype.csv\"\n",
        "target_col = \"Cover_Type\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Forma original:\", df.shape)\n",
        "\n",
        "# 20k\n",
        "N = 20_000\n",
        "if len(df) > N:\n",
        "    df_20k = (df.groupby(target_col, group_keys=False)\n",
        "                .apply(lambda g: g.sample(n=min(len(g), max(1, int(len(g) * N / len(df)))), random_state=42))\n",
        "                .reset_index(drop=True))\n",
        "    if len(df_20k) > N:\n",
        "        df_20k = df_20k.sample(n=N, random_state=42)\n",
        "else:\n",
        "    df_20k = df.copy()\n",
        "\n",
        "print(\"Forma 20k:\", df_20k.shape)\n",
        "df_20k.to_csv(\"/content/covtype_20k.csv\", index=False)\n",
        "\n",
        "# Definir X, y\n",
        "LABELS = np.sort(df_20k[target_col].unique())\n",
        "y = pd.Categorical(df_20k[target_col], categories=LABELS, ordered=True).codes\n",
        "num_labels = len(LABELS)\n",
        "print(\"Clases originales:\", LABELS)\n",
        "print(\"Clases codificadas (0..K-1):\", sorted(np.unique(y)))\n",
        "\n",
        "X = df_20k.drop(columns=[target_col]).to_numpy(dtype=float)\n",
        "mu = X.mean(axis=0)\n",
        "sigma = X.std(axis=0, ddof=0)\n",
        "sigma[sigma == 0] = 1.0\n",
        "X = (X - mu) / sigma\n",
        "\n",
        "# Split estándar\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(\"Forma:\", X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw2yVc8ESxgL",
        "outputId": "1d516f0f-ce34-4d56-f49d-256802d821ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.16204139 -1.166078    0.58137729  2.99345097 -1.36312617  0.47818423\n",
            " -0.63748858 -1.14396312 -0.15289832  0.08034289  0.40609041 -0.08182416\n",
            " -0.28935484 -0.24735266 -0.06761107 -0.08516423 -0.1078655  -0.10101525\n",
            " -0.04363264 -0.09204353  0.         -0.01732397 -0.01224898 -0.16387478\n",
            " -0.07053456 -0.52655121 -0.07089168 -0.04587625  0.         -0.14521559\n",
            " -0.09037128 -0.14841456 -0.08120252 -0.23147332 -0.01414426 -0.07124705\n",
            " -0.23963276 -0.15224087  0.         -0.02450348 -0.01581416 -0.0070716\n",
            "  1.3362727  -0.3296354  -0.06416613 -0.09422857 -0.08780519 -0.0070716\n",
            " -0.04065578 -0.01414426 -0.0187125  -0.10276878 -0.10276878 -0.09583578]\n",
            "[0 0 0 ... 6 6 6]\n"
          ]
        }
      ],
      "source": [
        "# Imprime la primera fila de características de la primera persona del dataset\n",
        "print(X[0,:])\n",
        "\n",
        "# Imprime el vector de etiquetas (y), que indica a qué grupo pertenece cada persona en el dataset\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HWF5C5Jt8wNb"
      },
      "outputs": [],
      "source": [
        "# Funcion normaliza datos\n",
        "def  featureNormalize(X):\n",
        "    X_norm = X.copy()\n",
        "    mu = np.zeros(X.shape[1])\n",
        "    sigma = np.zeros(X.shape[1])\n",
        "\n",
        "    mu = np.mean(X, axis = 0)\n",
        "    sigma = np.std(X, axis = 0)\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return X_norm, mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f4W6d-ya1ABV"
      },
      "outputs": [],
      "source": [
        "# LLAMAR A LA FUNCIÓN NORMALIZADORA\n",
        "X_norm, mu, sigma = featureNormalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lk8TeZPt5_S",
        "outputId": "6f5022fa-09d5-4a8b-a853-8dbe427f98ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.16204139 -1.166078    0.58137729  2.99345097 -1.36312617  0.47818423\n",
            " -0.63748858 -1.14396312 -0.15289832  0.08034289  0.40609041 -0.08182416\n",
            " -0.28935484 -0.24735266 -0.06761107 -0.08516423 -0.1078655  -0.10101525\n",
            " -0.04363264 -0.09204353         nan -0.01732397 -0.01224898 -0.16387478\n",
            " -0.07053456 -0.52655121 -0.07089168 -0.04587625         nan -0.14521559\n",
            " -0.09037128 -0.14841456 -0.08120252 -0.23147332 -0.01414426 -0.07124705\n",
            " -0.23963276 -0.15224087         nan -0.02450348 -0.01581416 -0.0070716\n",
            "  1.3362727  -0.3296354  -0.06416613 -0.09422857 -0.08780519 -0.0070716\n",
            " -0.04065578 -0.01414426 -0.0187125  -0.10276878 -0.10276878 -0.09583578]\n",
            "[0 0 0 ... 6 6 6]\n"
          ]
        }
      ],
      "source": [
        "print(X_norm[0,:])\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r9e6MRrW1G22"
      },
      "outputs": [],
      "source": [
        "# PREPARAR LOS DATOS PARA ENTRENAR CON DATOS NORMALIZADOS\n",
        "m, n = X.shape\n",
        "X = X_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0sB9Kyi8SxgN"
      },
      "outputs": [],
      "source": [
        "#FUNCIÓN SIGMOIDE\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fdCFEcXzGDsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calcularCosto(theta, X, y):\n",
        "    m = y.size\n",
        "\n",
        "    J = 0\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6WWZmMXnGL-w"
      },
      "outputs": [],
      "source": [
        "# ENTRENAMIENTO\n",
        "def descensoGradiente(theta, X, y, alpha, num_iters):\n",
        "    m = y.shape[0]\n",
        "    theta = theta.copy()\n",
        "\n",
        "    J_history = []\n",
        "\n",
        "    # Repetir\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(X.dot(theta.T))\n",
        "\n",
        "        # Fórmula para actualizar\n",
        "        theta = theta - (alpha / m) * (h - y).dot(X)\n",
        "\n",
        "        # Guardar costo de aprendizaje\n",
        "        J_history.append(calcularCosto(theta, X, y))\n",
        "\n",
        "    return theta, J_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w5S0OOswSxgN"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN NUEVA DE ENTRENAMIENTO (Con regularización)\n",
        "\n",
        "def lrCostFunction(theta, X, y, lambda_):\n",
        "\n",
        "    m = y.size\n",
        "\n",
        "    if y.dtype == bool:\n",
        "        y = y.astype(int)\n",
        "\n",
        "    J = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "\n",
        "    temp = theta\n",
        "    temp[0] = 0\n",
        "\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
        "\n",
        "    grad = (1 / m) * (h - y).dot(X)\n",
        "    grad = grad + (lambda_ / m) * temp\n",
        "\n",
        "    return J, grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T4I2f0hi_kYq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UbXIdt-_kYr"
      },
      "source": [
        "<h1 align=\"center\">FUNCION ONE VS ALL</h1>\n",
        "\n",
        "### Descripción\n",
        "El método **One vs All (OvA)** extiende la regresión logística binaria a problemas multiclase.  \n",
        "Se entrena un modelo por cada clase: cada modelo predice si un ejemplo pertenece o no a esa clase.  \n",
        "La clase final se elige tomando el modelo con la probabilidad más alta.\n",
        "\n",
        "---\n",
        "\n",
        "### Fórmulas\n",
        "\n",
        "**Hipótesis logística**\n",
        "\n",
        "hθ(x) = 1 / (1 + e^(-θᵀx))\n",
        "\n",
        "**Función de costo regularizada**\n",
        "\n",
        "J(θ) = -(1/m) Σ [ y(i) log(hθ(x(i))) + (1 - y(i)) log(1 - hθ(x(i))) ]   \n",
        "       + (λ / 2m) Σ θⱼ²   (para j ≥ 1)\n",
        "\n",
        "**Actualización por descenso de gradiente**\n",
        "\n",
        "θ := θ - α (1/m) Σ ( hθ(x(i)) - y(i) ) x(i)\n",
        "\n",
        "---\n",
        "\n",
        "### Procedimiento clásico\n",
        "\n",
        "1. Para cada clase c en el conjunto de etiquetas:  \n",
        "   - Convertir las etiquetas en binarias (1 si es la clase c, 0 en otro caso).  \n",
        "   - Entrenar un modelo de regresión logística mediante descenso de gradiente.  \n",
        "   - Guardar los parámetros obtenidos en θc.  \n",
        "\n",
        "2. Para clasificar un nuevo ejemplo:  \n",
        "   - Calcular la probabilidad de pertenecer a cada clase.  \n",
        "   - Seleccionar la clase con la probabilidad más alta.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4xM2MCvsGqYo"
      },
      "outputs": [],
      "source": [
        "# (One vs All)\n",
        "def OneVsAll(X, y, num_labels, lambda_):\n",
        "  alpha = 0.001\n",
        "  num_iters = 100000\n",
        "\n",
        "  m, n = X.shape\n",
        "  all_theta = np.zeros((num_labels, n + 1))\n",
        "\n",
        "  X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "  for c in np.arange(num_labels):\n",
        "      initial_theta = np.zeros(n + 1)\n",
        "\n",
        "      y_actual = np.where(y == c, 1, 0)\n",
        "\n",
        "      theta, J_history = descensoGradiente(initial_theta, X, y_actual, alpha, num_iters)\n",
        "\n",
        "      all_theta[c] = theta\n",
        "\n",
        "      pyplot.plot(np.arange(len(J_history)), J_history, lw=2)\n",
        "      pyplot.xlabel('Numero de iteraciones')\n",
        "      pyplot.ylabel('Costo J')\n",
        "\n",
        "  return all_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KqVj7I_kYs"
      },
      "source": [
        "<h1 align=\"center\">FUNCION ONE VS ALL OPTIMIZADO</h1>\n",
        "\n",
        "Esta versión implementa un optimizador avanzado (`scipy.optimize.minimize`) en lugar del descenso de gradiente manual.  \n",
        "Los parámetros θ convergen más rápido y de forma más estable.\n",
        "\n",
        "## Hipótesis logística\n",
        "\n",
        "$$\n",
        "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$$\n",
        "\n",
        "## Función de costo regularizada\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}\n",
        "\\Big[ y^{(i)} \\log(h_\\theta(x^{(i)})) +\n",
        "(1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\Big]\n",
        "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "## Ventajas del método optimizado\n",
        "- No requiere tasa de aprendizaje α  \n",
        "- Usa algoritmos de optimización eficientes (ej. gradiente conjugado)  \n",
        "- Converge en menos iteraciones\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "V0rOw5qhSxgN"
      },
      "outputs": [],
      "source": [
        "# (One vs All Optimizado)\n",
        "def OneVsAllOM(X, y, num_labels, lambda_):\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    all_theta = np.zeros((num_labels, n + 1))\n",
        "\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    for c in np.arange(num_labels):\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "        options = {'maxiter': 10}\n",
        "\n",
        "\n",
        "        res = optimize.minimize(lrCostFunction,\n",
        "            initial_theta,\n",
        "            (X, (y == c), lambda_),\n",
        "            jac=True,\n",
        "            method='CG',\n",
        "            options=options)\n",
        "\n",
        "        all_theta[c] = res.x\n",
        "\n",
        "    return all_theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6JbsLLMSxgO",
        "outputId": "6f4fa6ec-e119-4e1f-dfe2-26c70eb22cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Modelo entrenado! Forma: (7, 55)\n"
          ]
        }
      ],
      "source": [
        "# ¡ENTRENAR!\n",
        "lambda_ = 0.1\n",
        "\n",
        "num_labels = len(LABELS)\n",
        "\n",
        "all_theta = OneVsAllOM(X, y, num_labels, lambda_)\n",
        "print(f\"¡Modelo entrenado! Forma: {all_theta.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DdDZ2CJ2_kYt",
        "outputId": "b64848b6-c8e6-4331-e1e1-4c13bc2efc9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(all_theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLdhtxvL_kYu",
        "outputId": "19f93f40-93cb-43b6-a986-67cf1ad1dfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Modelo clásico entrenado! Forma: (7, 55)\n"
          ]
        }
      ],
      "source": [
        "#ENTRENA AL MODELO ONE VS ALL CLASICO\n",
        "\n",
        "def OneVsAll(X, y, num_labels, lambda_):\n",
        "    alpha = 0.01\n",
        "    num_iters = 1000\n",
        "    m, n = X.shape\n",
        "    all_theta = np.zeros((num_labels, n + 1))\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "    for c in np.arange(num_labels):\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "        y_actual = np.where(y == c, 1, 0)\n",
        "        theta, J_history = descensoGradiente(initial_theta, X, y_actual, alpha, num_iters)\n",
        "        all_theta[c] = theta\n",
        "\n",
        "    return all_theta\n",
        "\n",
        "all_theta_clasico = OneVsAll(X, y, num_labels, lambda_)\n",
        "print(f\"¡Modelo clásico entrenado! Forma: {all_theta_clasico.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1EfKaiEgtcw",
        "outputId": "ef8e3338-5b44-4034-dec6-348405645a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]\n",
            " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan]]\n"
          ]
        }
      ],
      "source": [
        "print(all_theta_clasico)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLqvbc_SxgO"
      },
      "source": [
        "<a id=\"section3\"></a>\n",
        "#### 1.4.1 Prediccion One-vs-all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PjFoFe1bSxgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ea8eb5-aa2c-4b50-a158-8e5166430345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19998, 54)\n",
            "Precision del conjuto de entrenamiento: 23.27%\n",
            "(45, 54)\n",
            "(45, 55)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# PREDICCIONES\n",
        "def predictOneVsAll(all_theta, X):\n",
        "\n",
        "    m = X.shape[0];\n",
        "    num_labels = all_theta.shape[0]\n",
        "\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)\n",
        "\n",
        "    return p\n",
        "\n",
        "\n",
        "print(X.shape)\n",
        "pred = predictOneVsAll(all_theta, X)\n",
        "print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))\n",
        "XPrueba = X[100:145, :].copy()\n",
        "print(XPrueba.shape)\n",
        "\n",
        "XPrueba = np.concatenate([np.ones((45, 1)), XPrueba], axis=1)\n",
        "print(XPrueba.shape)\n",
        "p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)\n",
        "print(p)\n",
        "\n",
        "print(y[100:145])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}