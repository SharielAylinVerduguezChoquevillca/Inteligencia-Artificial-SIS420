{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0398a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
      "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
      "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
      "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
      "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
      "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
      "\n",
      "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
      "0          3.33          3.11          0.22       1.94             3.12   \n",
      "1          3.23          3.02          0.21       1.89             3.03   \n",
      "2          3.21          3.01          0.20       1.88             3.02   \n",
      "3          3.26          3.07          0.19       1.92             3.08   \n",
      "4          3.27          3.08          0.19       1.92             3.09   \n",
      "\n",
      "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0       1307.75      1.03           1.75     152.3  \n",
      "1       1309.80      0.72           1.50     136.1  \n",
      "2       1310.24      0.19           0.63     171.6  \n",
      "3       1309.19      0.34           0.50     198.0  \n",
      "4       1309.00      0.32           0.63     214.3  \n",
      "\n",
      "Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420551 entries, 0 to 420550\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Date Time        420551 non-null  object \n",
      " 1   p (mbar)         420551 non-null  float64\n",
      " 2   T (degC)         420551 non-null  float64\n",
      " 3   Tpot (K)         420551 non-null  float64\n",
      " 4   Tdew (degC)      420551 non-null  float64\n",
      " 5   rh (%)           420551 non-null  float64\n",
      " 6   VPmax (mbar)     420551 non-null  float64\n",
      " 7   VPact (mbar)     420551 non-null  float64\n",
      " 8   VPdef (mbar)     420551 non-null  float64\n",
      " 9   sh (g/kg)        420551 non-null  float64\n",
      " 10  H2OC (mmol/mol)  420551 non-null  float64\n",
      " 11  rho (g/m**3)     420551 non-null  float64\n",
      " 12  wv (m/s)         420551 non-null  float64\n",
      " 13  max. wv (m/s)    420551 non-null  float64\n",
      " 14  wd (deg)         420551 non-null  float64\n",
      "dtypes: float64(14), object(1)\n",
      "memory usage: 48.1+ MB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas:\n",
      "                    count         mean        std      min      25%      50%  \\\n",
      "p (mbar)         420551.0   989.212776   8.358481   913.60   984.20   989.58   \n",
      "T (degC)         420551.0     9.450147   8.423365   -23.01     3.36     9.42   \n",
      "Tpot (K)         420551.0   283.492743   8.504471   250.60   277.43   283.47   \n",
      "Tdew (degC)      420551.0     4.955854   6.730674   -25.01     0.24     5.22   \n",
      "rh (%)           420551.0    76.008259  16.476175    12.95    65.21    79.30   \n",
      "VPmax (mbar)     420551.0    13.576251   7.739020     0.95     7.78    11.82   \n",
      "VPact (mbar)     420551.0     9.533756   4.184164     0.79     6.21     8.86   \n",
      "VPdef (mbar)     420551.0     4.042412   4.896851     0.00     0.87     2.19   \n",
      "sh (g/kg)        420551.0     6.022408   2.656139     0.50     3.92     5.59   \n",
      "H2OC (mmol/mol)  420551.0     9.640223   4.235395     0.80     6.29     8.96   \n",
      "rho (g/m**3)     420551.0  1216.062748  39.975208  1059.45  1187.49  1213.79   \n",
      "wv (m/s)         420551.0     1.702224  65.446714 -9999.00     0.99     1.76   \n",
      "max. wv (m/s)    420551.0     3.056555  69.016932 -9999.00     1.76     2.96   \n",
      "wd (deg)         420551.0   174.743738  86.681693     0.00   124.90   198.10   \n",
      "\n",
      "                     75%      max  \n",
      "p (mbar)          994.72  1015.35  \n",
      "T (degC)           15.47    37.28  \n",
      "Tpot (K)          289.53   311.34  \n",
      "Tdew (degC)        10.07    23.11  \n",
      "rh (%)             89.40   100.00  \n",
      "VPmax (mbar)       17.60    63.77  \n",
      "VPact (mbar)       12.35    28.32  \n",
      "VPdef (mbar)        5.30    46.01  \n",
      "sh (g/kg)           7.80    18.13  \n",
      "H2OC (mmol/mol)    12.49    28.82  \n",
      "rho (g/m**3)     1242.77  1393.54  \n",
      "wv (m/s)            2.86    28.49  \n",
      "max. wv (m/s)       4.74    23.50  \n",
      "wd (deg)          234.10   360.00  \n",
      "\n",
      "Columnas del dataset:\n",
      "Index(['Date Time', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)',\n",
      "       'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)',\n",
      "       'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)',\n",
      "       'wd (deg)'],\n",
      "      dtype='object')\n",
      "\n",
      "Valores nulos por columna:\n",
      "Date Time          0\n",
      "p (mbar)           0\n",
      "T (degC)           0\n",
      "Tpot (K)           0\n",
      "Tdew (degC)        0\n",
      "rh (%)             0\n",
      "VPmax (mbar)       0\n",
      "VPact (mbar)       0\n",
      "VPdef (mbar)       0\n",
      "sh (g/kg)          0\n",
      "H2OC (mmol/mol)    0\n",
      "rho (g/m**3)       0\n",
      "wv (m/s)           0\n",
      "max. wv (m/s)      0\n",
      "wd (deg)           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta del dataset\n",
    "file_path = \"C:/archive/jena_climate_2009_2016.csv\"\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Información general (columnas, tipos de datos, valores nulos, memoria)\n",
    "print(\"\\nInformación general:\")\n",
    "print(df.info())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "print(df.describe().T)\n",
    "\n",
    "# Ver nombres de columnas\n",
    "print(\"\\nColumnas del dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Revisar si hay valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356870b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes después de limpiar:\n",
      "p (mbar)           0\n",
      "T (degC)           0\n",
      "Tpot (K)           0\n",
      "Tdew (degC)        0\n",
      "rh (%)             0\n",
      "VPmax (mbar)       0\n",
      "VPact (mbar)       0\n",
      "VPdef (mbar)       0\n",
      "sh (g/kg)          0\n",
      "H2OC (mmol/mol)    0\n",
      "rho (g/m**3)       0\n",
      "wv (m/s)           0\n",
      "max. wv (m/s)      0\n",
      "wd (deg)           0\n",
      "dtype: int64\n",
      "\n",
      " Dataset limpio guardado en: C:/archive/jena_climate_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar dataset original\n",
    "file_path = \"C:/archive/jena_climate_2009_2016.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convertir Date Time a datetime y ponerlo como índice\n",
    "df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"], format=\"%d.%m.%Y %H:%M:%S\")\n",
    "df = df.set_index(\"Date Time\")\n",
    "\n",
    "# Reemplazar valores erróneos (-9999.0) por NaN\n",
    "df[\"wv (m/s)\"] = df[\"wv (m/s)\"].replace(-9999.0, np.nan)\n",
    "df[\"max. wv (m/s)\"] = df[\"max. wv (m/s)\"].replace(-9999.0, np.nan)\n",
    "\n",
    "# Interpolación para completar los valores faltantes\n",
    "df[\"wv (m/s)\"] = df[\"wv (m/s)\"].interpolate()\n",
    "df[\"max. wv (m/s)\"] = df[\"max. wv (m/s)\"].interpolate()\n",
    "\n",
    "# Verificar que ya no hay valores erróneos\n",
    "print(\"Valores faltantes después de limpiar:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Guardar dataset limpio\n",
    "output_path = \"C:/archive/jena_climate_clean.csv\"\n",
    "df.to_csv(output_path)\n",
    "\n",
    "print(f\"\\n Dataset limpio guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89fc725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 30, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder LSTM para secuencias univariadas o multivariadas.\n",
    "    Input shape: (batch, seq_len, n_features)\n",
    "    Salida: tupla (h_n, c_n) para inicializar el decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features: int = 1, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        # outputs: (batch, seq_len, hidden_size)\n",
    "        return h_n, c_n\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder LSTM que genera una secuencia paso-a-paso.\n",
    "    Cada paso recibe como entrada la predicción previa (o el valor real si teacher forcing).\n",
    "    \"\"\"\n",
    "    def __init__(self, out_features: int = 1, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=out_features,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, y_prev, hidden):\n",
    "        \"\"\"\n",
    "        y_prev: (batch, 1, out_features)  -> input para el paso actual\n",
    "        hidden: tuple (h_n, c_n) como proveniente del encoder o del paso anterior\n",
    "        retorna: y_pred (batch, 1, out_features), hidden actualizado\n",
    "        \"\"\"\n",
    "        out, hidden = self.lstm(y_prev, hidden)   # out: (batch, 1, hidden_size)\n",
    "        y_pred = self.fc(out)                     # (batch, 1, out_features)\n",
    "        return y_pred, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Seq2Seq LSTM completo (Encoder + Decoder).\n",
    "    - input_features: número de features en la entrada (1 para serie univariada)\n",
    "    - output_features: número de features en la salida (1 para predicción univariada)\n",
    "    - hidden_size, num_layers, dropout: arquitectura LSTM\n",
    "    Forward:\n",
    "      src: (batch, src_len, input_features)\n",
    "      target_len: pasos a predecir (int)\n",
    "      teacher_forcing_ratio: entre 0 y 1. Si >0 y se pasa target_seq, utiliza teacher forcing aleatoriamente.\n",
    "      target_seq (opcional): (batch, target_len, output_features) para teacher forcing.\n",
    "    Devuelve:\n",
    "      outputs: (batch, target_len, output_features)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_features: int = 1,\n",
    "                 output_features: int = 1,\n",
    "                 hidden_size: int = 128,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.2,\n",
    "                 init_scale: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = LSTMEncoder(n_features=input_features,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   num_layers=num_layers,\n",
    "                                   dropout=dropout)\n",
    "        self.decoder = LSTMDecoder(out_features=output_features,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   num_layers=num_layers,\n",
    "                                   dropout=dropout)\n",
    "        # inicialización simple y controlable\n",
    "        self._init_weights(init_scale)\n",
    "\n",
    "    def _init_weights(self, scale: float = 0.1):\n",
    "        # inicializa pesos lineales y LSTM para estabilidad al inicio\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p, gain=1.0)\n",
    "            else:\n",
    "                # biases a cero\n",
    "                nn.init.constant_(p, 0.0)\n",
    "        # reducir escala de fc final si existe\n",
    "        try:\n",
    "            nn.init.uniform_(self.decoder.fc.weight, -scale, scale)\n",
    "            if self.decoder.fc.bias is not None:\n",
    "                nn.init.constant_(self.decoder.fc.bias, 0.0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def forward(self, src, target_len: int, teacher_forcing_ratio: float = 0.0, target_seq: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        src: (batch, src_len, input_features)\n",
    "        target_len: int (horizonte de predicción)\n",
    "        teacher_forcing_ratio: probabilidad de usar el target real como entrada al decoder\n",
    "        target_seq: (batch, target_len, output_features) - opcional para teacher forcing\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        # encoder\n",
    "        h_n, c_n = self.encoder(src)  # cada uno: (num_layers, batch, hidden)\n",
    "        hidden = (h_n, c_n)\n",
    "\n",
    "        # primer input al decoder: último paso del src (última característica observada)\n",
    "        # si input_features != output_features, podría usarse una proyección; aquí asumimos compatible (o univariado)\n",
    "        y_prev = src[:, -1:, :self.decoder.fc.out_features]  # (batch, 1, out_features)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(target_len):\n",
    "            y_pred, hidden = self.decoder(y_prev, hidden)  # (batch,1,out_features)\n",
    "            outputs.append(y_pred)\n",
    "            # decidir teacher forcing\n",
    "            if (target_seq is not None) and (torch.rand(1).item() < teacher_forcing_ratio):\n",
    "                # usar el valor real (teacher forcing)\n",
    "                y_prev = target_seq[:, t:t+1, :]\n",
    "            else:\n",
    "                # usar la predicción como siguiente entrada (detach para evitar grafo innecesario)\n",
    "                y_prev = y_pred.detach()\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, target_len, out_features)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ejemplo:\n",
    "if __name__ == \"__main__\":\n",
    "    # prueba rápida de shapes\n",
    "    model = Seq2SeqLSTM(input_features=1, output_features=1, hidden_size=64, num_layers=2, dropout=0.1)\n",
    "    x = torch.randn(8, 30, 1)   # batch 8, 30 pasos de entrada, 1 feature\n",
    "    out = model(x, target_len=30, teacher_forcing_ratio=0.5, target_seq=torch.randn(8,30,1))\n",
    "    print(\"Output shape:\", out.shape)  # -> (8, 30, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e1205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Pasos por día: 144\n",
      "Secuencias creadas: (97985, 1008, 1) (97985, 1008, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   5%|▍         | 52/1072 [03:00<57:43,  3.40s/it]  "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Config reproducible\n",
    "SEED = 42\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Modelo Seq2Seq LSTM\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_features=1, output_features=1, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_size=input_features,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.decoder = nn.LSTM(input_size=output_features,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_size, output_features)\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if \"weight\" in name and p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(p, 0.0)\n",
    "\n",
    "    def forward(self, src, target_len, teacher_forcing_ratio=0.0, target_seq=None):\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        # Decoder: primer input\n",
    "        last_obs = src[:, -1:, :]\n",
    "        out_features = self.fc.out_features\n",
    "        if last_obs.size(-1) != out_features:\n",
    "            proj = nn.Linear(last_obs.size(-1), out_features).to(device)\n",
    "            with torch.no_grad():\n",
    "                y_prev = proj(last_obs)\n",
    "        else:\n",
    "            y_prev = last_obs[:, :, :out_features]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(target_len):\n",
    "            out, (hidden, cell) = self.decoder(y_prev, (hidden, cell))\n",
    "            y_pred = self.fc(out)\n",
    "            outputs.append(y_pred)\n",
    "            if self.training and target_seq is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                y_prev = target_seq[:, t:t+1, :]\n",
    "            else:\n",
    "                y_prev = y_pred.detach()\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "DATA_PATH = Path(\"C:/archive/jena_climate_clean.csv\")\n",
    "TARGET_COL = \"T (degC)\"\n",
    "\n",
    "INPUT_DAYS = 7    \n",
    "OUTPUT_DAYS = 7\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 10       \n",
    "PATIENCE = 5\n",
    "CLIP_GRAD = 1.0\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", DEVICE)\n",
    "\n",
    "# Cargar CSV\n",
    "df = pd.read_csv(DATA_PATH, index_col=0, parse_dates=True)\n",
    "df = df.iloc[:100000] \n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Columna objetivo '{TARGET_COL}' no encontrada.\")\n",
    "\n",
    "# Inferir frecuencia y pasos por día\n",
    "freq = pd.infer_freq(df.index)\n",
    "if freq is None:\n",
    "    diffs = df.index.to_series().diff().dropna()\n",
    "    delta_sec = diffs.mode().iloc[0].total_seconds()\n",
    "else:\n",
    "    freq_tdelta = pd.tseries.frequencies.to_offset(freq)\n",
    "    delta_sec = freq_tdelta.delta.total_seconds()\n",
    "steps_per_day = int(86400 / delta_sec)\n",
    "print(f\"Pasos por día: {steps_per_day}\")\n",
    "\n",
    "INPUT_LEN = INPUT_DAYS * steps_per_day\n",
    "OUTPUT_LEN = OUTPUT_DAYS * steps_per_day\n",
    "\n",
    "# 5) Escalado\n",
    "values = df[[TARGET_COL]].values.astype(np.float32)\n",
    "scaler = MinMaxScaler()\n",
    "values_scaled = scaler.fit_transform(values)\n",
    "\n",
    "# 6) Crear secuencias\n",
    "def create_sequences(data, input_len, output_len):\n",
    "    xs, ys = [], []\n",
    "    max_start = len(data) - input_len - output_len\n",
    "    for i in range(max_start + 1):\n",
    "        xs.append(data[i:i+input_len])\n",
    "        ys.append(data[i+input_len:i+input_len+output_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, Y = create_sequences(values_scaled, INPUT_LEN, OUTPUT_LEN)\n",
    "print(\"Secuencias creadas:\", X.shape, Y.shape)\n",
    "\n",
    "# Split\n",
    "n = len(X)\n",
    "train_end = int(n*0.7)\n",
    "val_end = int(n*0.85)\n",
    "\n",
    "X_train, Y_train = X[:train_end], Y[:train_end]\n",
    "X_val, Y_val = X[train_end:val_end], Y[train_end:val_end]\n",
    "X_test, Y_test = X[val_end:], Y[val_end:]\n",
    "\n",
    "# Dataset y DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.Y[idx]\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(TimeSeriesDataset(X_val, Y_val), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(TimeSeriesDataset(X_test, Y_test), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Modelo, loss, optimizador\n",
    "model = Seq2SeqLSTM(input_features=1, output_features=1, hidden_size=HIDDEN_SIZE,\n",
    "                    num_layers=NUM_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "\n",
    "# Entrenamiento con tqdm y early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "best_path = Path(\"best_seq2seq_fast.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for Xb, Yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Train]\", leave=False):\n",
    "        Xb, Yb = Xb.to(DEVICE), Yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb, target_len=OUTPUT_LEN, teacher_forcing_ratio=0.5, target_seq=Yb)\n",
    "        loss = criterion(outputs, Yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * Xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [Val]\", leave=False):\n",
    "            Xb, Yb = Xb.to(DEVICE), Yb.to(DEVICE)\n",
    "            preds = model(Xb, target_len=OUTPUT_LEN, teacher_forcing_ratio=0.0)\n",
    "            loss = criterion(preds, Yb)\n",
    "            val_loss += loss.item() * Xb.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch:03d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val_loss - 1e-8:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "print(f\"Mejor modelo guardado en {best_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18883fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 1) Cargar el mejor modelo\n",
    "model.load_state_dict(torch.load(\"best_seq2seq_fast.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# 2) Obtener predicciones del test\n",
    "preds_list, trues_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, Yb in test_loader:\n",
    "        Xb, Yb = Xb.to(DEVICE), Yb.to(DEVICE)\n",
    "        preds = model(Xb, target_len=OUTPUT_LEN, teacher_forcing_ratio=0.0)\n",
    "        preds_list.append(preds.cpu().numpy())\n",
    "        trues_list.append(Yb.cpu().numpy())\n",
    "\n",
    "preds_arr = np.concatenate(preds_list, axis=0)  # (N_samples, OUTPUT_LEN, 1)\n",
    "trues_arr = np.concatenate(trues_list, axis=0)\n",
    "\n",
    "# 3) Desescalar\n",
    "preds_flat = preds_arr.reshape(-1, 1)\n",
    "trues_flat = trues_arr.reshape(-1, 1)\n",
    "\n",
    "preds_rescaled = scaler.inverse_transform(preds_flat).reshape(preds_arr.shape)\n",
    "trues_rescaled = scaler.inverse_transform(trues_flat).reshape(trues_arr.shape)\n",
    "\n",
    "# 4) Métricas finales\n",
    "mae_final = np.mean(np.abs(preds_rescaled - trues_rescaled))\n",
    "mse_final = np.mean((preds_rescaled - trues_rescaled)**2)\n",
    "\n",
    "print(f\"TEST -> MAE: {mae_final:.4f} | MSE: {mse_final:.4f}\")\n",
    "\n",
    "# 5) Gráficos para comparar\n",
    "\n",
    "# Comparar la primera muestra del test\n",
    "idx = 0\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(trues_rescaled[idx, :, 0], label=\"Real\", linewidth=2)\n",
    "plt.plot(preds_rescaled[idx, :, 0], label=\"Predicho\", linestyle='--')\n",
    "plt.title(f\"Predicción de {OUTPUT_DAYS} días - sample {idx}\")\n",
    "plt.xlabel(\"Paso temporal\")\n",
    "plt.ylabel(TARGET_COL)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6) Comparación general (promedio)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(np.mean(trues_rescaled, axis=0)[:,0], label=\"Real promedio\")\n",
    "plt.plot(np.mean(preds_rescaled, axis=0)[:,0], label=\"Predicho promedio\", linestyle='--')\n",
    "plt.title(f\"Promedio sobre todo el test ({len(trues_rescaled)} samples)\")\n",
    "plt.xlabel(\"Paso temporal\")\n",
    "plt.ylabel(TARGET_COL)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
